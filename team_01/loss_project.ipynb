{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/idlbatmansquad/env/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "# reshape to be [samples][width][height][channels]\n",
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1).astype('float32')\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1).astype('float32')\n",
    "# normalize inputs from 0-255 to 0-1\n",
    "X_train = X_train / 255.\n",
    "X_test = X_test / 255.\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We put the model.fit inside the loop because we want to obtain an array of the errors with the same loss function\n",
    "#for calculate the mean and the standard deviation of our loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We first try with the categorical crossentropy loss function because it's one of the typical error function that NN use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Error: 0.97%\n",
      "Baseline Error: 0.98%\n",
      "Baseline Error: 1.01%\n",
      "Baseline Error: 0.93%\n",
      "Baseline Error: 0.97%\n",
      "Baseline Error: 0.98%\n",
      "Baseline Error: 1.01%\n",
      "Baseline Error: 1.00%\n",
      "Baseline Error: 1.09%\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (5, 5), input_shape=(28, 28, 1), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "scores_vector = []\n",
    "for iteration in range(0,9):\n",
    "    model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, batch_size=200, verbose=0)\n",
    "    scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "    scores_vector.append((100-scores[1]*100))\n",
    "    print(\"Baseline Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean value upon 10 iterations: 0.9933333333333346\n",
      "Standard Deviation upon 10 iterations: 0.04136557881997198\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean value upon 10 iterations: \" + str(np.mean(scores_vector)))\n",
    "print(\"Standard Deviation upon 10 iterations: \" + str(np.std(scores_vector)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we try with the MSE loss function because it's one of the typical error function that NN use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Error: 0.94%\n",
      "Baseline Error: 1.02%\n",
      "Baseline Error: 0.98%\n",
      "Baseline Error: 0.90%\n",
      "Baseline Error: 0.79%\n",
      "Baseline Error: 1.05%\n",
      "Baseline Error: 0.97%\n",
      "Baseline Error: 0.97%\n",
      "Baseline Error: 0.90%\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (5, 5), input_shape=(28, 28, 1), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "scores_vector = []\n",
    "for iteration in range(0,9):\n",
    "    model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, batch_size=200, verbose=0)\n",
    "    scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "    scores_vector.append((100-scores[1]*100))\n",
    "    print(\"Baseline Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean value upon 10 iterations: 0.9466666666666678\n",
      "Standard Deviation upon 10 iterations: 0.07241853660799837\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean value upon 10 iterations: \" + str(np.mean(scores_vector)))\n",
    "print(\"Standard Deviation upon 10 iterations: \" + str(np.std(scores_vector)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we try with the squared hinge function because it is a function commonly used in classifiers, \n",
    "#especially in SVMs and when there are only two classes. \n",
    "#However, we have used it because it is also a good measure when you want to compare a class against another\n",
    "#or a class against all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Error: 0.95%\n",
      "Baseline Error: 1.03%\n",
      "Baseline Error: 1.01%\n",
      "Baseline Error: 1.05%\n",
      "Baseline Error: 1.06%\n",
      "Baseline Error: 0.98%\n",
      "Baseline Error: 1.02%\n",
      "Baseline Error: 0.98%\n",
      "Baseline Error: 1.01%\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (5, 5), input_shape=(28, 28, 1), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.compile(loss='squared_hinge', optimizer='adam', metrics=['accuracy'])\n",
    "scores_vector = []\n",
    "for iteration in range(0,9):\n",
    "    model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, batch_size=200, verbose=0)\n",
    "    scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "    scores_vector.append((100-scores[1]*100))\n",
    "    print(\"Baseline Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean value upon 10 iterations: 1.0100000000000005\n",
      "Standard Deviation upon 10 iterations: 0.033333333333334755\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean value upon 10 iterations: \" + str(np.mean(scores_vector)))\n",
    "print(\"Standard Deviation upon 10 iterations: \" + str(np.std(scores_vector)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we try with the squared Kullbackâ€“Leibler divergence function because it's a good measure for the deviation\n",
    "#of the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Error: 0.91%\n",
      "Baseline Error: 0.93%\n",
      "Baseline Error: 0.97%\n",
      "Baseline Error: 0.90%\n",
      "Baseline Error: 0.92%\n",
      "Baseline Error: 0.92%\n",
      "Baseline Error: 1.03%\n",
      "Baseline Error: 0.95%\n",
      "Baseline Error: 1.07%\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (5, 5), input_shape=(28, 28, 1), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.compile(loss='kullback_leibler_divergence', optimizer='adam', metrics=['accuracy'])\n",
    "scores_vector = []\n",
    "for iteration in range(0,9):\n",
    "    model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, batch_size=200, verbose=0)\n",
    "    scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "    scores_vector.append((100-scores[1]*100))\n",
    "    print(\"Baseline Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean value upon 10 iterations: 0.9555555555555549\n",
      "Standard Deviation upon 10 iterations: 0.05499719409228882\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean value upon 10 iterations: \" + str(np.mean(scores_vector)))\n",
    "print(\"Standard Deviation upon 10 iterations: \" + str(np.std(scores_vector)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We observe both results, we conclude the Mean Squared Error is a better loss function in this case as it gives a lower mean."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
